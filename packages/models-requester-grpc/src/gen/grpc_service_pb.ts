// Copyright 2020-2024, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions
// are met:
//  * Redistributions of source code must retain the above copyright
//    notice, this list of conditions and the following disclaimer.
//  * Redistributions in binary form must reproduce the above copyright
//    notice, this list of conditions and the following disclaimer in the
//    documentation and/or other materials provided with the distribution.
//  * Neither the name of NVIDIA CORPORATION nor the names of its
//    contributors may be used to endorse or promote products derived
//    from this software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
// PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
// OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

// @generated by protoc-gen-es v2.2.3 with parameter "target=ts"
// @generated from file grpc_service.proto (package inference, syntax proto3)
/* eslint-disable */

import type { GenFile, GenMessage, GenService } from "@bufbuild/protobuf/codegenv1";
import { fileDesc, messageDesc, serviceDesc } from "@bufbuild/protobuf/codegenv1";
import type { ModelConfig } from "./model_config_pb";
import { file_model_config } from "./model_config_pb";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file grpc_service.proto.
 */
export const file_grpc_service: GenFile = /*@__PURE__*/
  fileDesc("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", [file_model_config]);

/**
 * @@
 * @@.. cpp:var:: message ServerLiveRequest
 * @@
 * @@   Request message for ServerLive.
 * @@
 *
 * @generated from message inference.ServerLiveRequest
 */
export type ServerLiveRequest = Message<"inference.ServerLiveRequest"> & {
};

/**
 * Describes the message inference.ServerLiveRequest.
 * Use `create(ServerLiveRequestSchema)` to create a new message.
 */
export const ServerLiveRequestSchema: GenMessage<ServerLiveRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 0);

/**
 * @@
 * @@.. cpp:var:: message ServerLiveResponse
 * @@
 * @@   Response message for ServerLive.
 * @@
 *
 * @generated from message inference.ServerLiveResponse
 */
export type ServerLiveResponse = Message<"inference.ServerLiveResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: bool live
   * @@
   * @@     True if the inference server is live, false it not live.
   * @@
   *
   * @generated from field: bool live = 1;
   */
  live: boolean;
};

/**
 * Describes the message inference.ServerLiveResponse.
 * Use `create(ServerLiveResponseSchema)` to create a new message.
 */
export const ServerLiveResponseSchema: GenMessage<ServerLiveResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 1);

/**
 * @@
 * @@.. cpp:var:: message ServerReadyRequest
 * @@
 * @@   Request message for ServerReady.
 * @@
 *
 * @generated from message inference.ServerReadyRequest
 */
export type ServerReadyRequest = Message<"inference.ServerReadyRequest"> & {
};

/**
 * Describes the message inference.ServerReadyRequest.
 * Use `create(ServerReadyRequestSchema)` to create a new message.
 */
export const ServerReadyRequestSchema: GenMessage<ServerReadyRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 2);

/**
 * @@
 * @@.. cpp:var:: message ServerReadyResponse
 * @@
 * @@   Response message for ServerReady.
 * @@
 *
 * @generated from message inference.ServerReadyResponse
 */
export type ServerReadyResponse = Message<"inference.ServerReadyResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: bool ready
   * @@
   * @@     True if the inference server is ready, false it not ready.
   * @@
   *
   * @generated from field: bool ready = 1;
   */
  ready: boolean;
};

/**
 * Describes the message inference.ServerReadyResponse.
 * Use `create(ServerReadyResponseSchema)` to create a new message.
 */
export const ServerReadyResponseSchema: GenMessage<ServerReadyResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 3);

/**
 * @@
 * @@.. cpp:var:: message ModelReadyRequest
 * @@
 * @@   Request message for ModelReady.
 * @@
 *
 * @generated from message inference.ModelReadyRequest
 */
export type ModelReadyRequest = Message<"inference.ModelReadyRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the model to check for readiness.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@  .. cpp:var:: string version
   * @@
   * @@     The version of the model to check for readiness. If not given the
   * @@     server will choose a version based on the model and internal policy.
   * @@
   *
   * @generated from field: string version = 2;
   */
  version: string;
};

/**
 * Describes the message inference.ModelReadyRequest.
 * Use `create(ModelReadyRequestSchema)` to create a new message.
 */
export const ModelReadyRequestSchema: GenMessage<ModelReadyRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 4);

/**
 * @@
 * @@.. cpp:var:: message ModelReadyResponse
 * @@
 * @@   Response message for ModelReady.
 * @@
 *
 * @generated from message inference.ModelReadyResponse
 */
export type ModelReadyResponse = Message<"inference.ModelReadyResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: bool ready
   * @@
   * @@     True if the model is ready, false it not ready.
   * @@
   *
   * @generated from field: bool ready = 1;
   */
  ready: boolean;
};

/**
 * Describes the message inference.ModelReadyResponse.
 * Use `create(ModelReadyResponseSchema)` to create a new message.
 */
export const ModelReadyResponseSchema: GenMessage<ModelReadyResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 5);

/**
 * @@
 * @@.. cpp:var:: message ServerMetadataRequest
 * @@
 * @@   Request message for ServerMetadata.
 * @@
 *
 * @generated from message inference.ServerMetadataRequest
 */
export type ServerMetadataRequest = Message<"inference.ServerMetadataRequest"> & {
};

/**
 * Describes the message inference.ServerMetadataRequest.
 * Use `create(ServerMetadataRequestSchema)` to create a new message.
 */
export const ServerMetadataRequestSchema: GenMessage<ServerMetadataRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 6);

/**
 * @@
 * @@.. cpp:var:: message ServerMetadataResponse
 * @@
 * @@   Response message for ServerMetadata.
 * @@
 *
 * @generated from message inference.ServerMetadataResponse
 */
export type ServerMetadataResponse = Message<"inference.ServerMetadataResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The server name.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@
   * @@  .. cpp:var:: string version
   * @@
   * @@     The server version.
   * @@
   *
   * @generated from field: string version = 2;
   */
  version: string;

  /**
   * @@
   * @@  .. cpp:var:: string extensions (repeated)
   * @@
   * @@     The extensions supported by the server.
   * @@
   *
   * @generated from field: repeated string extensions = 3;
   */
  extensions: string[];
};

/**
 * Describes the message inference.ServerMetadataResponse.
 * Use `create(ServerMetadataResponseSchema)` to create a new message.
 */
export const ServerMetadataResponseSchema: GenMessage<ServerMetadataResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 7);

/**
 * @@
 * @@.. cpp:var:: message ModelMetadataRequest
 * @@
 * @@   Request message for ModelMetadata.
 * @@
 *
 * @generated from message inference.ModelMetadataRequest
 */
export type ModelMetadataRequest = Message<"inference.ModelMetadataRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the model.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@  .. cpp:var:: string version
   * @@
   * @@     The version of the model to check for readiness. If not
   * @@     given the server will choose a version based on the
   * @@     model and internal policy.
   * @@
   *
   * @generated from field: string version = 2;
   */
  version: string;
};

/**
 * Describes the message inference.ModelMetadataRequest.
 * Use `create(ModelMetadataRequestSchema)` to create a new message.
 */
export const ModelMetadataRequestSchema: GenMessage<ModelMetadataRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 8);

/**
 * @@
 * @@.. cpp:var:: message ModelMetadataResponse
 * @@
 * @@   Response message for ModelMetadata.
 * @@
 *
 * @generated from message inference.ModelMetadataResponse
 */
export type ModelMetadataResponse = Message<"inference.ModelMetadataResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The model name.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@
   * @@  .. cpp:var:: string versions (repeated)
   * @@
   * @@     The versions of the model.
   * @@
   *
   * @generated from field: repeated string versions = 2;
   */
  versions: string[];

  /**
   * @@
   * @@  .. cpp:var:: string platform
   * @@
   * @@     The model's platform.
   * @@
   *
   * @generated from field: string platform = 3;
   */
  platform: string;

  /**
   * @@
   * @@  .. cpp:var:: TensorMetadata inputs (repeated)
   * @@
   * @@     The model's inputs.
   * @@
   *
   * @generated from field: repeated inference.ModelMetadataResponse.TensorMetadata inputs = 4;
   */
  inputs: ModelMetadataResponse_TensorMetadata[];

  /**
   * @@
   * @@  .. cpp:var:: TensorMetadata outputs (repeated)
   * @@
   * @@     The model's outputs.
   * @@
   *
   * @generated from field: repeated inference.ModelMetadataResponse.TensorMetadata outputs = 5;
   */
  outputs: ModelMetadataResponse_TensorMetadata[];
};

/**
 * Describes the message inference.ModelMetadataResponse.
 * Use `create(ModelMetadataResponseSchema)` to create a new message.
 */
export const ModelMetadataResponseSchema: GenMessage<ModelMetadataResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 9);

/**
 * @@
 * @@  .. cpp:var:: message TensorMetadata
 * @@
 * @@     Metadata for a tensor.
 * @@
 *
 * @generated from message inference.ModelMetadataResponse.TensorMetadata
 */
export type ModelMetadataResponse_TensorMetadata = Message<"inference.ModelMetadataResponse.TensorMetadata"> & {
  /**
   * @@
   * @@    .. cpp:var:: string name
   * @@
   * @@       The tensor name.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@
   * @@    .. cpp:var:: string datatype
   * @@
   * @@       The tensor data type.
   * @@
   *
   * @generated from field: string datatype = 2;
   */
  datatype: string;

  /**
   * @@
   * @@    .. cpp:var:: int64 shape (repeated)
   * @@
   * @@       The tensor shape. A variable-size dimension is represented
   * @@       by a -1 value.
   * @@
   *
   * @generated from field: repeated int64 shape = 3;
   */
  shape: bigint[];
};

/**
 * Describes the message inference.ModelMetadataResponse.TensorMetadata.
 * Use `create(ModelMetadataResponse_TensorMetadataSchema)` to create a new message.
 */
export const ModelMetadataResponse_TensorMetadataSchema: GenMessage<ModelMetadataResponse_TensorMetadata> = /*@__PURE__*/
  messageDesc(file_grpc_service, 9, 0);

/**
 * @@
 * @@.. cpp:var:: message InferParameter
 * @@
 * @@   An inference parameter value.
 * @@
 *
 * @generated from message inference.InferParameter
 */
export type InferParameter = Message<"inference.InferParameter"> & {
  /**
   * @@  .. cpp:var:: oneof parameter_choice
   * @@
   * @@     The parameter value can be a string, an int64,
   * @@     an uint64, a double, or a boolean
   * @@
   * @@     Note: double and uint64 are currently
   * @@           placeholders for future use and
   * @@           are not supported for custom parameters
   * @@
   *
   * @generated from oneof inference.InferParameter.parameter_choice
   */
  parameterChoice: {
    /**
     * @@    .. cpp:var:: bool bool_param
     * @@
     * @@       A boolean parameter value.
     * @@
     *
     * @generated from field: bool bool_param = 1;
     */
    value: boolean;
    case: "boolParam";
  } | {
    /**
     * @@    .. cpp:var:: int64 int64_param
     * @@
     * @@       An int64 parameter value.
     * @@
     *
     * @generated from field: int64 int64_param = 2;
     */
    value: bigint;
    case: "int64Param";
  } | {
    /**
     * @@    .. cpp:var:: string string_param
     * @@
     * @@       A string parameter value.
     * @@
     *
     * @generated from field: string string_param = 3;
     */
    value: string;
    case: "stringParam";
  } | {
    /**
     * @@    .. cpp:var:: double double_param
     * @@
     * @@       A double parameter value.
     * @@
     *
     * @generated from field: double double_param = 4;
     */
    value: number;
    case: "doubleParam";
  } | {
    /**
     * @@    .. cpp:var:: uint64 uint64_param
     * @@
     * @@       A uint64 parameter value.
     * @@
     * @@       Not supported for custom parameters
     * @@
     *
     * @generated from field: uint64 uint64_param = 5;
     */
    value: bigint;
    case: "uint64Param";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message inference.InferParameter.
 * Use `create(InferParameterSchema)` to create a new message.
 */
export const InferParameterSchema: GenMessage<InferParameter> = /*@__PURE__*/
  messageDesc(file_grpc_service, 10);

/**
 * @@
 * @@.. cpp:var:: message InferTensorContents
 * @@
 * @@   The data contained in a tensor represented by the repeated type
 * @@   that matches the tensor's data type. Protobuf oneof is not used
 * @@   because oneofs cannot contain repeated fields.
 * @@
 *
 * @generated from message inference.InferTensorContents
 */
export type InferTensorContents = Message<"inference.InferTensorContents"> & {
  /**
   * @@
   * @@  .. cpp:var:: bool bool_contents (repeated)
   * @@
   * @@     Representation for BOOL data type. The size must match what is
   * @@     expected by the tensor's shape. The contents must be the flattened,
   * @@     one-dimensional, row-major order of the tensor elements.
   * @@
   *
   * @generated from field: repeated bool bool_contents = 1;
   */
  boolContents: boolean[];

  /**
   * @@
   * @@  .. cpp:var:: int32 int_contents (repeated)
   * @@
   * @@     Representation for INT8, INT16, and INT32 data types. The size
   * @@     must match what is expected by the tensor's shape. The contents
   * @@     must be the flattened, one-dimensional, row-major order of the
   * @@     tensor elements.
   * @@
   *
   * @generated from field: repeated int32 int_contents = 2;
   */
  intContents: number[];

  /**
   * @@
   * @@  .. cpp:var:: int64 int64_contents (repeated)
   * @@
   * @@     Representation for INT64 data types. The size must match what
   * @@     is expected by the tensor's shape. The contents must be the
   * @@     flattened, one-dimensional, row-major order of the tensor elements.
   * @@
   *
   * @generated from field: repeated int64 int64_contents = 3;
   */
  int64Contents: bigint[];

  /**
   * @@
   * @@  .. cpp:var:: uint32 uint_contents (repeated)
   * @@
   * @@     Representation for UINT8, UINT16, and UINT32 data types. The size
   * @@     must match what is expected by the tensor's shape. The contents
   * @@     must be the flattened, one-dimensional, row-major order of the
   * @@     tensor elements.
   * @@
   *
   * @generated from field: repeated uint32 uint_contents = 4;
   */
  uintContents: number[];

  /**
   * @@
   * @@  .. cpp:var:: uint64 uint64_contents (repeated)
   * @@
   * @@     Representation for UINT64 data types. The size must match what
   * @@     is expected by the tensor's shape. The contents must be the
   * @@     flattened, one-dimensional, row-major order of the tensor elements.
   * @@
   *
   * @generated from field: repeated uint64 uint64_contents = 5;
   */
  uint64Contents: bigint[];

  /**
   * @@
   * @@  .. cpp:var:: float fp32_contents (repeated)
   * @@
   * @@     Representation for FP32 data type. The size must match what is
   * @@     expected by the tensor's shape. The contents must be the flattened,
   * @@     one-dimensional, row-major order of the tensor elements.
   * @@
   *
   * @generated from field: repeated float fp32_contents = 6;
   */
  fp32Contents: number[];

  /**
   * @@
   * @@  .. cpp:var:: double fp64_contents (repeated)
   * @@
   * @@     Representation for FP64 data type. The size must match what is
   * @@     expected by the tensor's shape. The contents must be the flattened,
   * @@     one-dimensional, row-major order of the tensor elements.
   * @@
   *
   * @generated from field: repeated double fp64_contents = 7;
   */
  fp64Contents: number[];

  /**
   * @@
   * @@  .. cpp:var:: bytes bytes_contents (repeated)
   * @@
   * @@     Representation for BYTES data type. The size must match what is
   * @@     expected by the tensor's shape. The contents must be the flattened,
   * @@     one-dimensional, row-major order of the tensor elements.
   * @@
   *
   * @generated from field: repeated bytes bytes_contents = 8;
   */
  bytesContents: Uint8Array[];
};

/**
 * Describes the message inference.InferTensorContents.
 * Use `create(InferTensorContentsSchema)` to create a new message.
 */
export const InferTensorContentsSchema: GenMessage<InferTensorContents> = /*@__PURE__*/
  messageDesc(file_grpc_service, 11);

/**
 * @@
 * @@.. cpp:var:: message ModelInferRequest
 * @@
 * @@   Request message for ModelInfer.
 * @@
 *
 * @generated from message inference.ModelInferRequest
 */
export type ModelInferRequest = Message<"inference.ModelInferRequest"> & {
  /**
   * @@  .. cpp:var:: string model_name
   * @@
   * @@     The name of the model to use for inferencing.
   * @@
   *
   * @generated from field: string model_name = 1;
   */
  modelName: string;

  /**
   * @@  .. cpp:var:: string model_version
   * @@
   * @@     The version of the model to use for inference. If not
   * @@     given the latest/most-recent version of the model is used.
   * @@
   *
   * @generated from field: string model_version = 2;
   */
  modelVersion: string;

  /**
   * @@  .. cpp:var:: string id
   * @@
   * @@     Optional identifier for the request. If specified will be
   * @@     returned in the response.
   * @@
   *
   * @generated from field: string id = 3;
   */
  id: string;

  /**
   * @@  .. cpp:var:: map<string,InferParameter> parameters
   * @@
   * @@     Optional inference parameters.
   * @@
   *
   * @generated from field: map<string, inference.InferParameter> parameters = 4;
   */
  parameters: { [key: string]: InferParameter };

  /**
   * @@
   * @@  .. cpp:var:: InferInputTensor inputs (repeated)
   * @@
   * @@     The input tensors for the inference.
   * @@
   *
   * @generated from field: repeated inference.ModelInferRequest.InferInputTensor inputs = 5;
   */
  inputs: ModelInferRequest_InferInputTensor[];

  /**
   * @@
   * @@  .. cpp:var:: InferRequestedOutputTensor outputs (repeated)
   * @@
   * @@     The requested output tensors for the inference. Optional, if not
   * @@     specified all outputs specified in the model config will be
   * @@     returned.
   * @@
   *
   * @generated from field: repeated inference.ModelInferRequest.InferRequestedOutputTensor outputs = 6;
   */
  outputs: ModelInferRequest_InferRequestedOutputTensor[];

  /**
   * @@
   * @@  .. cpp:var:: bytes raw_input_contents
   * @@
   * @@     The data contained in an input tensor can be represented in
   * @@     "raw" bytes form or in the repeated type that matches the
   * @@     tensor's data type. Using the "raw" bytes form will
   * @@     typically allow higher performance due to the way protobuf
   * @@     allocation and reuse interacts with GRPC. For example, see
   * @@     https://github.com/grpc/grpc/issues/23231.
   * @@
   * @@     To use the raw representation 'raw_input_contents' must be
   * @@     initialized with data for each tensor in the same order as
   * @@     'inputs'. For each tensor, the size of this content must
   * @@     match what is expected by the tensor's shape and data
   * @@     type. The raw data must be the flattened, one-dimensional,
   * @@     row-major order of the tensor elements without any stride
   * @@     or padding between the elements. Note that the FP16 and BF16 data
   * @@     types must be represented as raw content as there is no
   * @@     specific data type for a 16-bit float type.
   * @@
   * @@     If this field is specified then InferInputTensor::contents
   * @@     must not be specified for any input tensor.
   * @@
   *
   * @generated from field: repeated bytes raw_input_contents = 7;
   */
  rawInputContents: Uint8Array[];
};

/**
 * Describes the message inference.ModelInferRequest.
 * Use `create(ModelInferRequestSchema)` to create a new message.
 */
export const ModelInferRequestSchema: GenMessage<ModelInferRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 12);

/**
 * @@
 * @@  .. cpp:var:: message InferInputTensor
 * @@
 * @@     An input tensor for an inference request.
 * @@
 *
 * @generated from message inference.ModelInferRequest.InferInputTensor
 */
export type ModelInferRequest_InferInputTensor = Message<"inference.ModelInferRequest.InferInputTensor"> & {
  /**
   * @@
   * @@    .. cpp:var:: string name
   * @@
   * @@       The tensor name.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@
   * @@    .. cpp:var:: string datatype
   * @@
   * @@       The tensor data type.
   * @@
   *
   * @generated from field: string datatype = 2;
   */
  datatype: string;

  /**
   * @@
   * @@    .. cpp:var:: int64 shape (repeated)
   * @@
   * @@       The tensor shape.
   * @@
   *
   * @generated from field: repeated int64 shape = 3;
   */
  shape: bigint[];

  /**
   * @@    .. cpp:var:: map<string,InferParameter> parameters
   * @@
   * @@       Optional inference input tensor parameters.
   * @@
   *
   * @generated from field: map<string, inference.InferParameter> parameters = 4;
   */
  parameters: { [key: string]: InferParameter };

  /**
   * @@    .. cpp:var:: InferTensorContents contents
   * @@
   * @@       The tensor contents using a data-type format. This field
   * @@       must not be specified if tensor contents are being specified
   * @@       in ModelInferRequest.raw_input_contents.
   * @@
   *
   * @generated from field: inference.InferTensorContents contents = 5;
   */
  contents?: InferTensorContents;
};

/**
 * Describes the message inference.ModelInferRequest.InferInputTensor.
 * Use `create(ModelInferRequest_InferInputTensorSchema)` to create a new message.
 */
export const ModelInferRequest_InferInputTensorSchema: GenMessage<ModelInferRequest_InferInputTensor> = /*@__PURE__*/
  messageDesc(file_grpc_service, 12, 0);

/**
 * @@
 * @@  .. cpp:var:: message InferRequestedOutputTensor
 * @@
 * @@     An output tensor requested for an inference request.
 * @@
 *
 * @generated from message inference.ModelInferRequest.InferRequestedOutputTensor
 */
export type ModelInferRequest_InferRequestedOutputTensor = Message<"inference.ModelInferRequest.InferRequestedOutputTensor"> & {
  /**
   * @@
   * @@    .. cpp:var:: string name
   * @@
   * @@       The tensor name.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@    .. cpp:var:: map<string,InferParameter> parameters
   * @@
   * @@       Optional requested output tensor parameters.
   * @@
   *
   * @generated from field: map<string, inference.InferParameter> parameters = 2;
   */
  parameters: { [key: string]: InferParameter };
};

/**
 * Describes the message inference.ModelInferRequest.InferRequestedOutputTensor.
 * Use `create(ModelInferRequest_InferRequestedOutputTensorSchema)` to create a new message.
 */
export const ModelInferRequest_InferRequestedOutputTensorSchema: GenMessage<ModelInferRequest_InferRequestedOutputTensor> = /*@__PURE__*/
  messageDesc(file_grpc_service, 12, 1);

/**
 * @@
 * @@.. cpp:var:: message ModelInferResponse
 * @@
 * @@   Response message for ModelInfer.
 * @@
 *
 * @generated from message inference.ModelInferResponse
 */
export type ModelInferResponse = Message<"inference.ModelInferResponse"> & {
  /**
   * @@  .. cpp:var:: string model_name
   * @@
   * @@     The name of the model used for inference.
   * @@
   *
   * @generated from field: string model_name = 1;
   */
  modelName: string;

  /**
   * @@  .. cpp:var:: string model_version
   * @@
   * @@     The version of the model used for inference.
   * @@
   *
   * @generated from field: string model_version = 2;
   */
  modelVersion: string;

  /**
   * @@  .. cpp:var:: string id
   * @@
   * @@     The id of the inference request if one was specified.
   * @@
   *
   * @generated from field: string id = 3;
   */
  id: string;

  /**
   * @@  .. cpp:var:: map<string,InferParameter> parameters
   * @@
   * @@     Optional inference response parameters.
   * @@
   *
   * @generated from field: map<string, inference.InferParameter> parameters = 4;
   */
  parameters: { [key: string]: InferParameter };

  /**
   * @@
   * @@  .. cpp:var:: InferOutputTensor outputs (repeated)
   * @@
   * @@     The output tensors holding inference results.
   * @@
   *
   * @generated from field: repeated inference.ModelInferResponse.InferOutputTensor outputs = 5;
   */
  outputs: ModelInferResponse_InferOutputTensor[];

  /**
   * @@
   * @@  .. cpp:var:: bytes raw_output_contents
   * @@
   * @@     The data contained in an output tensor can be represented in
   * @@     "raw" bytes form or in the repeated type that matches the
   * @@     tensor's data type. Using the "raw" bytes form will
   * @@     typically allow higher performance due to the way protobuf
   * @@     allocation and reuse interacts with GRPC. For example, see
   * @@     https://github.com/grpc/grpc/issues/23231.
   * @@
   * @@     To use the raw representation 'raw_output_contents' must be
   * @@     initialized with data for each tensor in the same order as
   * @@     'outputs'. For each tensor, the size of this content must
   * @@     match what is expected by the tensor's shape and data
   * @@     type. The raw data must be the flattened, one-dimensional,
   * @@     row-major order of the tensor elements without any stride
   * @@     or padding between the elements. Note that the FP16 and BF16 data
   * @@     types must be represented as raw content as there is no
   * @@     specific data type for a 16-bit float type.
   * @@
   * @@     If this field is specified then InferOutputTensor::contents
   * @@     must not be specified for any output tensor.
   * @@
   *
   * @generated from field: repeated bytes raw_output_contents = 6;
   */
  rawOutputContents: Uint8Array[];
};

/**
 * Describes the message inference.ModelInferResponse.
 * Use `create(ModelInferResponseSchema)` to create a new message.
 */
export const ModelInferResponseSchema: GenMessage<ModelInferResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 13);

/**
 * @@
 * @@  .. cpp:var:: message InferOutputTensor
 * @@
 * @@     An output tensor returned for an inference request.
 * @@
 *
 * @generated from message inference.ModelInferResponse.InferOutputTensor
 */
export type ModelInferResponse_InferOutputTensor = Message<"inference.ModelInferResponse.InferOutputTensor"> & {
  /**
   * @@
   * @@    .. cpp:var:: string name
   * @@
   * @@       The tensor name.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@
   * @@    .. cpp:var:: string datatype
   * @@
   * @@       The tensor data type.
   * @@
   *
   * @generated from field: string datatype = 2;
   */
  datatype: string;

  /**
   * @@
   * @@    .. cpp:var:: int64 shape (repeated)
   * @@
   * @@       The tensor shape.
   * @@
   *
   * @generated from field: repeated int64 shape = 3;
   */
  shape: bigint[];

  /**
   * @@    .. cpp:var:: map<string,InferParameter> parameters
   * @@
   * @@       Optional output tensor parameters.
   * @@
   *
   * @generated from field: map<string, inference.InferParameter> parameters = 4;
   */
  parameters: { [key: string]: InferParameter };

  /**
   * @@    .. cpp:var:: InferTensorContents contents
   * @@
   * @@       The tensor contents using a data-type format. This field
   * @@       must not be specified if tensor contents are being specified
   * @@       in ModelInferResponse.raw_output_contents.
   * @@
   *
   * @generated from field: inference.InferTensorContents contents = 5;
   */
  contents?: InferTensorContents;
};

/**
 * Describes the message inference.ModelInferResponse.InferOutputTensor.
 * Use `create(ModelInferResponse_InferOutputTensorSchema)` to create a new message.
 */
export const ModelInferResponse_InferOutputTensorSchema: GenMessage<ModelInferResponse_InferOutputTensor> = /*@__PURE__*/
  messageDesc(file_grpc_service, 13, 0);

/**
 * @@
 * @@.. cpp:var:: message ModelStreamInferResponse
 * @@
 * @@   Response message for ModelStreamInfer.
 * @@
 *
 * @generated from message inference.ModelStreamInferResponse
 */
export type ModelStreamInferResponse = Message<"inference.ModelStreamInferResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: string error_message
   * @@
   * @@     The message describing the error. The empty message
   * @@     indicates the inference was successful without errors.
   * @@
   *
   * @generated from field: string error_message = 1;
   */
  errorMessage: string;

  /**
   * @@
   * @@  .. cpp:var:: ModelInferResponse infer_response
   * @@
   * @@     Holds the results of the request.
   * @@
   *
   * @generated from field: inference.ModelInferResponse infer_response = 2;
   */
  inferResponse?: ModelInferResponse;
};

/**
 * Describes the message inference.ModelStreamInferResponse.
 * Use `create(ModelStreamInferResponseSchema)` to create a new message.
 */
export const ModelStreamInferResponseSchema: GenMessage<ModelStreamInferResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 14);

/**
 * @@
 * @@.. cpp:var:: message ModelConfigRequest
 * @@
 * @@   Request message for ModelConfig.
 * @@
 *
 * @generated from message inference.ModelConfigRequest
 */
export type ModelConfigRequest = Message<"inference.ModelConfigRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the model.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@  .. cpp:var:: string version
   * @@
   * @@     The version of the model. If not given the model version
   * @@     is selected automatically based on the version policy.
   * @@
   *
   * @generated from field: string version = 2;
   */
  version: string;
};

/**
 * Describes the message inference.ModelConfigRequest.
 * Use `create(ModelConfigRequestSchema)` to create a new message.
 */
export const ModelConfigRequestSchema: GenMessage<ModelConfigRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 15);

/**
 * @@
 * @@.. cpp:var:: message ModelConfigResponse
 * @@
 * @@   Response message for ModelConfig.
 * @@
 *
 * @generated from message inference.ModelConfigResponse
 */
export type ModelConfigResponse = Message<"inference.ModelConfigResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: ModelConfig config
   * @@
   * @@     The model configuration.
   * @@
   *
   * @generated from field: inference.ModelConfig config = 1;
   */
  config?: ModelConfig;
};

/**
 * Describes the message inference.ModelConfigResponse.
 * Use `create(ModelConfigResponseSchema)` to create a new message.
 */
export const ModelConfigResponseSchema: GenMessage<ModelConfigResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 16);

/**
 * @@
 * @@.. cpp:var:: message ModelStatisticsRequest
 * @@
 * @@   Request message for ModelStatistics.
 * @@
 *
 * @generated from message inference.ModelStatisticsRequest
 */
export type ModelStatisticsRequest = Message<"inference.ModelStatisticsRequest"> & {
  /**
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the model. If not given returns statistics for
   * @@     all models.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@  .. cpp:var:: string version
   * @@
   * @@     The version of the model. If not given returns statistics for
   * @@     all model versions.
   * @@
   *
   * @generated from field: string version = 2;
   */
  version: string;
};

/**
 * Describes the message inference.ModelStatisticsRequest.
 * Use `create(ModelStatisticsRequestSchema)` to create a new message.
 */
export const ModelStatisticsRequestSchema: GenMessage<ModelStatisticsRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 17);

/**
 * @@
 * @@.. cpp:var:: message StatisticDuration
 * @@
 * @@   Statistic recording a cumulative duration metric.
 * @@
 *
 * @generated from message inference.StatisticDuration
 */
export type StatisticDuration = Message<"inference.StatisticDuration"> & {
  /**
   * @@  .. cpp:var:: uint64 count
   * @@
   * @@     Cumulative number of times this metric occurred.
   * @@
   *
   * @generated from field: uint64 count = 1;
   */
  count: bigint;

  /**
   * @@  .. cpp:var:: uint64 total_time_ns
   * @@
   * @@     Total collected duration of this metric in nanoseconds.
   * @@
   *
   * @generated from field: uint64 ns = 2;
   */
  ns: bigint;
};

/**
 * Describes the message inference.StatisticDuration.
 * Use `create(StatisticDurationSchema)` to create a new message.
 */
export const StatisticDurationSchema: GenMessage<StatisticDuration> = /*@__PURE__*/
  messageDesc(file_grpc_service, 18);

/**
 * @@
 * @@.. cpp:var:: message InferStatistics
 * @@
 * @@   Inference statistics.
 * @@
 *
 * @generated from message inference.InferStatistics
 */
export type InferStatistics = Message<"inference.InferStatistics"> & {
  /**
   * @@  .. cpp:var:: StatisticDuration success
   * @@
   * @@     Cumulative count and duration for successful inference
   * @@     request. The "success" count and cumulative duration includes
   * @@     cache hits.
   * @@
   *
   * @generated from field: inference.StatisticDuration success = 1;
   */
  success?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration fail
   * @@
   * @@     Cumulative count and duration for failed inference
   * @@     request.
   * @@
   *
   * @generated from field: inference.StatisticDuration fail = 2;
   */
  fail?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration queue
   * @@
   * @@     The count and cumulative duration that inference requests wait in
   * @@     scheduling or other queues. The "queue" count and cumulative
   * @@     duration includes cache hits.
   * @@
   *
   * @generated from field: inference.StatisticDuration queue = 3;
   */
  queue?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration compute_input
   * @@
   * @@     The count and cumulative duration to prepare input tensor data as
   * @@     required by the model framework / backend. For example, this duration
   * @@     should include the time to copy input tensor data to the GPU.
   * @@     The "compute_input" count and cumulative duration do not account for
   * @@     requests that were a cache hit. See the "cache_hit" field for more
   * @@     info.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_input = 4;
   */
  computeInput?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration compute_infer
   * @@
   * @@     The count and cumulative duration to execute the model.
   * @@     The "compute_infer" count and cumulative duration do not account for
   * @@     requests that were a cache hit. See the "cache_hit" field for more
   * @@     info.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_infer = 5;
   */
  computeInfer?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration compute_output
   * @@
   * @@     The count and cumulative duration to extract output tensor data
   * @@     produced by the model framework / backend. For example, this duration
   * @@     should include the time to copy output tensor data from the GPU.
   * @@     The "compute_output" count and cumulative duration do not account for
   * @@     requests that were a cache hit. See the "cache_hit" field for more
   * @@     info.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_output = 6;
   */
  computeOutput?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration cache_hit
   * @@
   * @@     The count of response cache hits and cumulative duration to lookup
   * @@     and extract output tensor data from the Response Cache on a cache
   * @@     hit. For example, this duration should include the time to copy
   * @@     output tensor data from the Response Cache to the response object.
   * @@     On cache hits, triton does not need to go to the model/backend
   * @@     for the output tensor data, so the "compute_input", "compute_infer",
   * @@     and "compute_output" fields are not updated. Assuming the response
   * @@     cache is enabled for a given model, a cache hit occurs for a
   * @@     request to that model when the request metadata (model name,
   * @@     model version, model inputs) hashes to an existing entry in the
   * @@     cache. On a cache miss, the request hash and response output tensor
   * @@     data is added to the cache. See response cache docs for more info:
   * @@
   * https://github.com/triton-inference-server/server/blob/main/docs/response_cache.md
   * @@
   *
   * @generated from field: inference.StatisticDuration cache_hit = 7;
   */
  cacheHit?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration cache_miss
   * @@
   * @@     The count of response cache misses and cumulative duration to lookup
   * @@     and insert output tensor data from the computed response to the
   * cache.
   * @@     For example, this duration should include the time to copy
   * @@     output tensor data from the response object to the Response Cache.
   * @@     Assuming the response cache is enabled for a given model, a cache
   * @@     miss occurs for a request to that model when the request metadata
   * @@     does NOT hash to an existing entry in the cache. See the response
   * @@     cache docs for more info:
   * @@
   * https://github.com/triton-inference-server/server/blob/main/docs/response_cache.md
   * @@
   *
   * @generated from field: inference.StatisticDuration cache_miss = 8;
   */
  cacheMiss?: StatisticDuration;
};

/**
 * Describes the message inference.InferStatistics.
 * Use `create(InferStatisticsSchema)` to create a new message.
 */
export const InferStatisticsSchema: GenMessage<InferStatistics> = /*@__PURE__*/
  messageDesc(file_grpc_service, 19);

/**
 * @@
 * @@.. cpp:var:: message InferResponseStatistics
 * @@
 * @@   Statistics per response.
 * @@
 *
 * @generated from message inference.InferResponseStatistics
 */
export type InferResponseStatistics = Message<"inference.InferResponseStatistics"> & {
  /**
   * @@  .. cpp:var:: StatisticDuration compute_infer
   * @@
   * @@     The count and cumulative duration to compute a response.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_infer = 1;
   */
  computeInfer?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration compute_output
   * @@
   * @@     The count and cumulative duration to extract the output tensors of a
   * @@     response.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_output = 2;
   */
  computeOutput?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration success
   * @@
   * @@     The count and cumulative duration for successful responses.
   * @@
   *
   * @generated from field: inference.StatisticDuration success = 3;
   */
  success?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration fail
   * @@
   * @@     The count and cumulative duration for failed responses.
   * @@
   *
   * @generated from field: inference.StatisticDuration fail = 4;
   */
  fail?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration empty_response
   * @@
   * @@     The count and cumulative duration for empty responses.
   * @@
   *
   * @generated from field: inference.StatisticDuration empty_response = 5;
   */
  emptyResponse?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration cancel
   * @@
   * @@     The count and cumulative duration, for cleaning up resources held by
   * @@     a cancelled request, for cancelled responses.
   * @@
   *
   * @generated from field: inference.StatisticDuration cancel = 6;
   */
  cancel?: StatisticDuration;
};

/**
 * Describes the message inference.InferResponseStatistics.
 * Use `create(InferResponseStatisticsSchema)` to create a new message.
 */
export const InferResponseStatisticsSchema: GenMessage<InferResponseStatistics> = /*@__PURE__*/
  messageDesc(file_grpc_service, 20);

/**
 * @@
 * @@.. cpp:var:: message InferBatchStatistics
 * @@
 * @@   Inference batch statistics.
 * @@
 *
 * @generated from message inference.InferBatchStatistics
 */
export type InferBatchStatistics = Message<"inference.InferBatchStatistics"> & {
  /**
   * @@  .. cpp:var:: uint64 batch_size
   * @@
   * @@     The size of the batch.
   * @@
   *
   * @generated from field: uint64 batch_size = 1;
   */
  batchSize: bigint;

  /**
   * @@  .. cpp:var:: StatisticDuration compute_input
   * @@
   * @@     The count and cumulative duration to prepare input tensor data as
   * @@     required by the model framework / backend with the given batch size.
   * @@     For example, this duration should include the time to copy input
   * @@     tensor data to the GPU.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_input = 2;
   */
  computeInput?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration compute_infer
   * @@
   * @@     The count and cumulative duration to execute the model with the given
   * @@     batch size.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_infer = 3;
   */
  computeInfer?: StatisticDuration;

  /**
   * @@  .. cpp:var:: StatisticDuration compute_output
   * @@
   * @@     The count and cumulative duration to extract output tensor data
   * @@     produced by the model framework / backend with the given batch size.
   * @@     For example, this duration should include the time to copy output
   * @@     tensor data from the GPU.
   * @@
   *
   * @generated from field: inference.StatisticDuration compute_output = 4;
   */
  computeOutput?: StatisticDuration;
};

/**
 * Describes the message inference.InferBatchStatistics.
 * Use `create(InferBatchStatisticsSchema)` to create a new message.
 */
export const InferBatchStatisticsSchema: GenMessage<InferBatchStatistics> = /*@__PURE__*/
  messageDesc(file_grpc_service, 21);

/**
 * @@
 * @@.. cpp:var:: message MemoryUsage
 * @@
 * @@   Memory usage.
 * @@
 *
 * @generated from message inference.MemoryUsage
 */
export type MemoryUsage = Message<"inference.MemoryUsage"> & {
  /**
   * @@  .. cpp:var:: string type
   * @@
   * @@     The type of memory, the value can be "CPU", "CPU_PINNED", "GPU".
   * @@
   *
   * @generated from field: string type = 1;
   */
  type: string;

  /**
   * @@  .. cpp:var:: int64 id
   * @@
   * @@     The id of the memory, typically used with "type" to identify
   * @@     a device that hosts the memory.
   * @@
   *
   * @generated from field: int64 id = 2;
   */
  id: bigint;

  /**
   * @@  .. cpp:var:: uint64 byte_size
   * @@
   * @@     The byte size of the memory.
   * @@
   *
   * @generated from field: uint64 byte_size = 3;
   */
  byteSize: bigint;
};

/**
 * Describes the message inference.MemoryUsage.
 * Use `create(MemoryUsageSchema)` to create a new message.
 */
export const MemoryUsageSchema: GenMessage<MemoryUsage> = /*@__PURE__*/
  messageDesc(file_grpc_service, 22);

/**
 * @@
 * @@.. cpp:var:: message ModelStatistics
 * @@
 * @@   Statistics for a specific model and version.
 * @@
 *
 * @generated from message inference.ModelStatistics
 */
export type ModelStatistics = Message<"inference.ModelStatistics"> & {
  /**
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the model. If not given returns statistics for all
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@  .. cpp:var:: string version
   * @@
   * @@     The version of the model.
   * @@
   *
   * @generated from field: string version = 2;
   */
  version: string;

  /**
   * @@  .. cpp:var:: uint64 last_inference
   * @@
   * @@     The timestamp of the last inference request made for this model,
   * @@     as milliseconds since the epoch.
   * @@
   *
   * @generated from field: uint64 last_inference = 3;
   */
  lastInference: bigint;

  /**
   * @@  .. cpp:var:: uint64 last_inference
   * @@
   * @@     The cumulative count of successful inference requests made for this
   * @@     model. Each inference in a batched request is counted as an
   * @@     individual inference. For example, if a client sends a single
   * @@     inference request with batch size 64, "inference_count" will be
   * @@     incremented by 64. Similarly, if a clients sends 64 individual
   * @@     requests each with batch size 1, "inference_count" will be
   * @@     incremented by 64. The "inference_count" value DOES NOT include
   * @@     cache hits.
   * @@
   *
   * @generated from field: uint64 inference_count = 4;
   */
  inferenceCount: bigint;

  /**
   * @@  .. cpp:var:: uint64 last_inference
   * @@
   * @@     The cumulative count of the number of successful inference executions
   * @@     performed for the model. When dynamic batching is enabled, a single
   * @@     model execution can perform inferencing for more than one inference
   * @@     request. For example, if a clients sends 64 individual requests each
   * @@     with batch size 1 and the dynamic batcher batches them into a single
   * @@     large batch for model execution then "execution_count" will be
   * @@     incremented by 1. If, on the other hand, the dynamic batcher is not
   * @@     enabled for that each of the 64 individual requests is executed
   * @@     independently, then "execution_count" will be incremented by 64.
   * @@     The "execution_count" value DOES NOT include cache hits.
   * @@
   *
   * @generated from field: uint64 execution_count = 5;
   */
  executionCount: bigint;

  /**
   * @@  .. cpp:var:: InferStatistics inference_stats
   * @@
   * @@     The aggregate statistics for the model/version.
   * @@
   *
   * @generated from field: inference.InferStatistics inference_stats = 6;
   */
  inferenceStats?: InferStatistics;

  /**
   * @@  .. cpp:var:: InferBatchStatistics batch_stats (repeated)
   * @@
   * @@     The aggregate statistics for each different batch size that is
   * @@     executed in the model. The batch statistics indicate how many actual
   * @@     model executions were performed and show differences due to different
   * @@     batch size (for example, larger batches typically take longer to
   * @@     compute).
   * @@
   *
   * @generated from field: repeated inference.InferBatchStatistics batch_stats = 7;
   */
  batchStats: InferBatchStatistics[];

  /**
   * @@  .. cpp:var:: MemoryUsage memory_usage (repeated)
   * @@
   * @@     The memory usage detected during model loading, which may be used to
   * @@     estimate the memory to be released once the model is unloaded. Note
   * @@     that the estimation is inferenced by the profiling tools and
   * @@     framework's memory schema, therefore it is advised to perform
   * @@     experiments to understand the scenario that the reported memory usage
   * @@     can be relied on. As a starting point, the GPU memory usage for
   * @@     models in ONNX Runtime backend and TensorRT backend is usually
   * @@     aligned.
   * @@
   *
   * @generated from field: repeated inference.MemoryUsage memory_usage = 8;
   */
  memoryUsage: MemoryUsage[];

  /**
   * @@  .. cpp:var:: map<string, InferResponseStatistics> response_stats
   * @@
   * @@     The key and value pairs for all responses statistics. The key is a
   * @@     string identifying a set of response statistics aggregated together
   * @@     (i.e. index of the response sent). The value is the aggregated
   * @@     response statistics.
   * @@
   *
   * @generated from field: map<string, inference.InferResponseStatistics> response_stats = 9;
   */
  responseStats: { [key: string]: InferResponseStatistics };
};

/**
 * Describes the message inference.ModelStatistics.
 * Use `create(ModelStatisticsSchema)` to create a new message.
 */
export const ModelStatisticsSchema: GenMessage<ModelStatistics> = /*@__PURE__*/
  messageDesc(file_grpc_service, 23);

/**
 * @@
 * @@.. cpp:var:: message ModelStatisticsResponse
 * @@
 * @@   Response message for ModelStatistics.
 * @@
 *
 * @generated from message inference.ModelStatisticsResponse
 */
export type ModelStatisticsResponse = Message<"inference.ModelStatisticsResponse"> & {
  /**
   * @@  .. cpp:var:: ModelStatistics model_stats (repeated)
   * @@
   * @@     Statistics for each requested model.
   * @@
   *
   * @generated from field: repeated inference.ModelStatistics model_stats = 1;
   */
  modelStats: ModelStatistics[];
};

/**
 * Describes the message inference.ModelStatisticsResponse.
 * Use `create(ModelStatisticsResponseSchema)` to create a new message.
 */
export const ModelStatisticsResponseSchema: GenMessage<ModelStatisticsResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 24);

/**
 * @@
 * @@.. cpp:var:: message ModelRepositoryParameter
 * @@
 * @@   An model repository parameter value.
 * @@
 *
 * @generated from message inference.ModelRepositoryParameter
 */
export type ModelRepositoryParameter = Message<"inference.ModelRepositoryParameter"> & {
  /**
   * @@  .. cpp:var:: oneof parameter_choice
   * @@
   * @@     The parameter value can be a string, an int64 or
   * @@     a boolean
   * @@
   *
   * @generated from oneof inference.ModelRepositoryParameter.parameter_choice
   */
  parameterChoice: {
    /**
     * @@    .. cpp:var:: bool bool_param
     * @@
     * @@       A boolean parameter value.
     * @@
     *
     * @generated from field: bool bool_param = 1;
     */
    value: boolean;
    case: "boolParam";
  } | {
    /**
     * @@    .. cpp:var:: int64 int64_param
     * @@
     * @@       An int64 parameter value.
     * @@
     *
     * @generated from field: int64 int64_param = 2;
     */
    value: bigint;
    case: "int64Param";
  } | {
    /**
     * @@    .. cpp:var:: string string_param
     * @@
     * @@       A string parameter value.
     * @@
     *
     * @generated from field: string string_param = 3;
     */
    value: string;
    case: "stringParam";
  } | {
    /**
     * @@    .. cpp:var:: bytes bytes_param
     * @@
     * @@       A bytes parameter value.
     * @@
     *
     * @generated from field: bytes bytes_param = 4;
     */
    value: Uint8Array;
    case: "bytesParam";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message inference.ModelRepositoryParameter.
 * Use `create(ModelRepositoryParameterSchema)` to create a new message.
 */
export const ModelRepositoryParameterSchema: GenMessage<ModelRepositoryParameter> = /*@__PURE__*/
  messageDesc(file_grpc_service, 25);

/**
 * @@
 * @@.. cpp:var:: message RepositoryIndexRequest
 * @@
 * @@   Request message for RepositoryIndex.
 * @@
 *
 * @generated from message inference.RepositoryIndexRequest
 */
export type RepositoryIndexRequest = Message<"inference.RepositoryIndexRequest"> & {
  /**
   * @@  .. cpp:var:: string repository_name
   * @@
   * @@     The name of the repository. If empty the index is returned
   * @@     for all repositories.
   * @@
   *
   * @generated from field: string repository_name = 1;
   */
  repositoryName: string;

  /**
   * @@  .. cpp:var:: bool ready
   * @@
   * @@     If true returned only models currently ready for inferencing.
   * @@
   *
   * @generated from field: bool ready = 2;
   */
  ready: boolean;
};

/**
 * Describes the message inference.RepositoryIndexRequest.
 * Use `create(RepositoryIndexRequestSchema)` to create a new message.
 */
export const RepositoryIndexRequestSchema: GenMessage<RepositoryIndexRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 26);

/**
 * @@
 * @@.. cpp:var:: message RepositoryIndexResponse
 * @@
 * @@   Response message for RepositoryIndex.
 * @@
 *
 * @generated from message inference.RepositoryIndexResponse
 */
export type RepositoryIndexResponse = Message<"inference.RepositoryIndexResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: ModelIndex models (repeated)
   * @@
   * @@     An index entry for each model.
   * @@
   *
   * @generated from field: repeated inference.RepositoryIndexResponse.ModelIndex models = 1;
   */
  models: RepositoryIndexResponse_ModelIndex[];
};

/**
 * Describes the message inference.RepositoryIndexResponse.
 * Use `create(RepositoryIndexResponseSchema)` to create a new message.
 */
export const RepositoryIndexResponseSchema: GenMessage<RepositoryIndexResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 27);

/**
 * @@
 * @@  .. cpp:var:: message ModelIndex
 * @@
 * @@     Index entry for a model.
 * @@
 *
 * @generated from message inference.RepositoryIndexResponse.ModelIndex
 */
export type RepositoryIndexResponse_ModelIndex = Message<"inference.RepositoryIndexResponse.ModelIndex"> & {
  /**
   * @@
   * @@    .. cpp:var:: string name
   * @@
   * @@       The name of the model.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@    .. cpp:var:: string version
   * @@
   * @@       The version of the model.
   * @@
   *
   * @generated from field: string version = 2;
   */
  version: string;

  /**
   * @@
   * @@    .. cpp:var:: string state
   * @@
   * @@       The state of the model.
   * @@
   *
   * @generated from field: string state = 3;
   */
  state: string;

  /**
   * @@
   * @@    .. cpp:var:: string reason
   * @@
   * @@       The reason, if any, that the model is in the given state.
   * @@
   *
   * @generated from field: string reason = 4;
   */
  reason: string;
};

/**
 * Describes the message inference.RepositoryIndexResponse.ModelIndex.
 * Use `create(RepositoryIndexResponse_ModelIndexSchema)` to create a new message.
 */
export const RepositoryIndexResponse_ModelIndexSchema: GenMessage<RepositoryIndexResponse_ModelIndex> = /*@__PURE__*/
  messageDesc(file_grpc_service, 27, 0);

/**
 * @@
 * @@.. cpp:var:: message RepositoryModelLoadRequest
 * @@
 * @@   Request message for RepositoryModelLoad.
 * @@
 *
 * @generated from message inference.RepositoryModelLoadRequest
 */
export type RepositoryModelLoadRequest = Message<"inference.RepositoryModelLoadRequest"> & {
  /**
   * @@  .. cpp:var:: string repository_name
   * @@
   * @@     The name of the repository to load from. If empty the model
   * @@     is loaded from any repository.
   * @@
   *
   * @generated from field: string repository_name = 1;
   */
  repositoryName: string;

  /**
   * @@  .. cpp:var:: string repository_name
   * @@
   * @@     The name of the model to load, or reload.
   * @@
   *
   * @generated from field: string model_name = 2;
   */
  modelName: string;

  /**
   * @@  .. cpp:var:: map<string,ModelRepositoryParameter> parameters
   * @@
   * @@     Optional model repository request parameters.
   * @@
   *
   * @generated from field: map<string, inference.ModelRepositoryParameter> parameters = 3;
   */
  parameters: { [key: string]: ModelRepositoryParameter };
};

/**
 * Describes the message inference.RepositoryModelLoadRequest.
 * Use `create(RepositoryModelLoadRequestSchema)` to create a new message.
 */
export const RepositoryModelLoadRequestSchema: GenMessage<RepositoryModelLoadRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 28);

/**
 * @@
 * @@.. cpp:var:: message RepositoryModelLoadResponse
 * @@
 * @@   Response message for RepositoryModelLoad.
 * @@
 *
 * @generated from message inference.RepositoryModelLoadResponse
 */
export type RepositoryModelLoadResponse = Message<"inference.RepositoryModelLoadResponse"> & {
};

/**
 * Describes the message inference.RepositoryModelLoadResponse.
 * Use `create(RepositoryModelLoadResponseSchema)` to create a new message.
 */
export const RepositoryModelLoadResponseSchema: GenMessage<RepositoryModelLoadResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 29);

/**
 * @@
 * @@.. cpp:var:: message RepositoryModelUnloadRequest
 * @@
 * @@   Request message for RepositoryModelUnload.
 * @@
 *
 * @generated from message inference.RepositoryModelUnloadRequest
 */
export type RepositoryModelUnloadRequest = Message<"inference.RepositoryModelUnloadRequest"> & {
  /**
   * @@  .. cpp:var:: string repository_name
   * @@
   * @@     The name of the repository from which the model was originally
   * @@     loaded. If empty the repository is not considered.
   * @@
   *
   * @generated from field: string repository_name = 1;
   */
  repositoryName: string;

  /**
   * @@  .. cpp:var:: string repository_name
   * @@
   * @@     The name of the model to unload.
   * @@
   *
   * @generated from field: string model_name = 2;
   */
  modelName: string;

  /**
   * @@  .. cpp:var:: map<string,ModelRepositoryParameter> parameters
   * @@
   * @@     Optional model repository request parameters.
   * @@
   *
   * @generated from field: map<string, inference.ModelRepositoryParameter> parameters = 3;
   */
  parameters: { [key: string]: ModelRepositoryParameter };
};

/**
 * Describes the message inference.RepositoryModelUnloadRequest.
 * Use `create(RepositoryModelUnloadRequestSchema)` to create a new message.
 */
export const RepositoryModelUnloadRequestSchema: GenMessage<RepositoryModelUnloadRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 30);

/**
 * @@
 * @@.. cpp:var:: message RepositoryModelUnloadResponse
 * @@
 * @@   Response message for RepositoryModelUnload.
 * @@
 *
 * @generated from message inference.RepositoryModelUnloadResponse
 */
export type RepositoryModelUnloadResponse = Message<"inference.RepositoryModelUnloadResponse"> & {
};

/**
 * Describes the message inference.RepositoryModelUnloadResponse.
 * Use `create(RepositoryModelUnloadResponseSchema)` to create a new message.
 */
export const RepositoryModelUnloadResponseSchema: GenMessage<RepositoryModelUnloadResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 31);

/**
 * @@
 * @@.. cpp:var:: message SystemSharedMemoryStatusRequest
 * @@
 * @@   Request message for SystemSharedMemoryStatus.
 * @@
 *
 * @generated from message inference.SystemSharedMemoryStatusRequest
 */
export type SystemSharedMemoryStatusRequest = Message<"inference.SystemSharedMemoryStatusRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the region to get status for. If empty the
   * @@     status is returned for all registered regions.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message inference.SystemSharedMemoryStatusRequest.
 * Use `create(SystemSharedMemoryStatusRequestSchema)` to create a new message.
 */
export const SystemSharedMemoryStatusRequestSchema: GenMessage<SystemSharedMemoryStatusRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 32);

/**
 * @@
 * @@.. cpp:var:: message SystemSharedMemoryStatusResponse
 * @@
 * @@   Response message for SystemSharedMemoryStatus.
 * @@
 *
 * @generated from message inference.SystemSharedMemoryStatusResponse
 */
export type SystemSharedMemoryStatusResponse = Message<"inference.SystemSharedMemoryStatusResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: map<string,RegionStatus> regions
   * @@
   * @@     Status for each of the registered regions, indexed by
   * @@     region name.
   * @@
   *
   * @generated from field: map<string, inference.SystemSharedMemoryStatusResponse.RegionStatus> regions = 1;
   */
  regions: { [key: string]: SystemSharedMemoryStatusResponse_RegionStatus };
};

/**
 * Describes the message inference.SystemSharedMemoryStatusResponse.
 * Use `create(SystemSharedMemoryStatusResponseSchema)` to create a new message.
 */
export const SystemSharedMemoryStatusResponseSchema: GenMessage<SystemSharedMemoryStatusResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 33);

/**
 * @@
 * @@  .. cpp:var:: message RegionStatus
 * @@
 * @@     Status for a shared memory region.
 * @@
 *
 * @generated from message inference.SystemSharedMemoryStatusResponse.RegionStatus
 */
export type SystemSharedMemoryStatusResponse_RegionStatus = Message<"inference.SystemSharedMemoryStatusResponse.RegionStatus"> & {
  /**
   * @@
   * @@    .. cpp:var:: string name
   * @@
   * @@       The name for the shared memory region.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@    .. cpp:var:: string shared_memory_key
   * @@
   * @@       The key of the underlying memory object that contains the
   * @@       shared memory region.
   * @@
   *
   * @generated from field: string key = 2;
   */
  key: string;

  /**
   * @@    .. cpp:var:: uint64 offset
   * @@
   * @@       Offset, in bytes, within the underlying memory object to
   * @@       the start of the shared memory region.
   * @@
   *
   * @generated from field: uint64 offset = 3;
   */
  offset: bigint;

  /**
   * @@    .. cpp:var:: uint64 byte_size
   * @@
   * @@       Size of the shared memory region, in bytes.
   * @@
   *
   * @generated from field: uint64 byte_size = 4;
   */
  byteSize: bigint;
};

/**
 * Describes the message inference.SystemSharedMemoryStatusResponse.RegionStatus.
 * Use `create(SystemSharedMemoryStatusResponse_RegionStatusSchema)` to create a new message.
 */
export const SystemSharedMemoryStatusResponse_RegionStatusSchema: GenMessage<SystemSharedMemoryStatusResponse_RegionStatus> = /*@__PURE__*/
  messageDesc(file_grpc_service, 33, 0);

/**
 * @@
 * @@.. cpp:var:: message SystemSharedMemoryRegisterRequest
 * @@
 * @@   Request message for SystemSharedMemoryRegister.
 * @@
 *
 * @generated from message inference.SystemSharedMemoryRegisterRequest
 */
export type SystemSharedMemoryRegisterRequest = Message<"inference.SystemSharedMemoryRegisterRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the region to register.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@  .. cpp:var:: string shared_memory_key
   * @@
   * @@     The key of the underlying memory object that contains the
   * @@     shared memory region.
   * @@
   *
   * @generated from field: string key = 2;
   */
  key: string;

  /**
   * @@  .. cpp:var:: uint64 offset
   * @@
   * @@     Offset, in bytes, within the underlying memory object to
   * @@     the start of the shared memory region.
   * @@
   *
   * @generated from field: uint64 offset = 3;
   */
  offset: bigint;

  /**
   * @@  .. cpp:var:: uint64 byte_size
   * @@
   * @@     Size of the shared memory region, in bytes.
   * @@
   *
   * @generated from field: uint64 byte_size = 4;
   */
  byteSize: bigint;
};

/**
 * Describes the message inference.SystemSharedMemoryRegisterRequest.
 * Use `create(SystemSharedMemoryRegisterRequestSchema)` to create a new message.
 */
export const SystemSharedMemoryRegisterRequestSchema: GenMessage<SystemSharedMemoryRegisterRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 34);

/**
 * @@
 * @@.. cpp:var:: message SystemSharedMemoryRegisterResponse
 * @@
 * @@   Response message for SystemSharedMemoryRegister.
 * @@
 *
 * @generated from message inference.SystemSharedMemoryRegisterResponse
 */
export type SystemSharedMemoryRegisterResponse = Message<"inference.SystemSharedMemoryRegisterResponse"> & {
};

/**
 * Describes the message inference.SystemSharedMemoryRegisterResponse.
 * Use `create(SystemSharedMemoryRegisterResponseSchema)` to create a new message.
 */
export const SystemSharedMemoryRegisterResponseSchema: GenMessage<SystemSharedMemoryRegisterResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 35);

/**
 * @@
 * @@.. cpp:var:: message SystemSharedMemoryUnregisterRequest
 * @@
 * @@   Request message for SystemSharedMemoryUnregister.
 * @@
 *
 * @generated from message inference.SystemSharedMemoryUnregisterRequest
 */
export type SystemSharedMemoryUnregisterRequest = Message<"inference.SystemSharedMemoryUnregisterRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the system region to unregister. If empty
   * @@     all system shared-memory regions are unregistered.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message inference.SystemSharedMemoryUnregisterRequest.
 * Use `create(SystemSharedMemoryUnregisterRequestSchema)` to create a new message.
 */
export const SystemSharedMemoryUnregisterRequestSchema: GenMessage<SystemSharedMemoryUnregisterRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 36);

/**
 * @@
 * @@.. cpp:var:: message SystemSharedMemoryUnregisterResponse
 * @@
 * @@   Response message for SystemSharedMemoryUnregister.
 * @@
 *
 * @generated from message inference.SystemSharedMemoryUnregisterResponse
 */
export type SystemSharedMemoryUnregisterResponse = Message<"inference.SystemSharedMemoryUnregisterResponse"> & {
};

/**
 * Describes the message inference.SystemSharedMemoryUnregisterResponse.
 * Use `create(SystemSharedMemoryUnregisterResponseSchema)` to create a new message.
 */
export const SystemSharedMemoryUnregisterResponseSchema: GenMessage<SystemSharedMemoryUnregisterResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 37);

/**
 * @@
 * @@.. cpp:var:: message CudaSharedMemoryStatusRequest
 * @@
 * @@   Request message for CudaSharedMemoryStatus.
 * @@
 *
 * @generated from message inference.CudaSharedMemoryStatusRequest
 */
export type CudaSharedMemoryStatusRequest = Message<"inference.CudaSharedMemoryStatusRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the region to get status for. If empty the
   * @@     status is returned for all registered regions.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message inference.CudaSharedMemoryStatusRequest.
 * Use `create(CudaSharedMemoryStatusRequestSchema)` to create a new message.
 */
export const CudaSharedMemoryStatusRequestSchema: GenMessage<CudaSharedMemoryStatusRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 38);

/**
 * @@
 * @@.. cpp:var:: message CudaSharedMemoryStatusResponse
 * @@
 * @@   Response message for CudaSharedMemoryStatus.
 * @@
 *
 * @generated from message inference.CudaSharedMemoryStatusResponse
 */
export type CudaSharedMemoryStatusResponse = Message<"inference.CudaSharedMemoryStatusResponse"> & {
  /**
   * @@
   * @@  .. cpp:var:: map<string,RegionStatus> regions
   * @@
   * @@     Status for each of the registered regions, indexed by
   * @@     region name.
   * @@
   *
   * @generated from field: map<string, inference.CudaSharedMemoryStatusResponse.RegionStatus> regions = 1;
   */
  regions: { [key: string]: CudaSharedMemoryStatusResponse_RegionStatus };
};

/**
 * Describes the message inference.CudaSharedMemoryStatusResponse.
 * Use `create(CudaSharedMemoryStatusResponseSchema)` to create a new message.
 */
export const CudaSharedMemoryStatusResponseSchema: GenMessage<CudaSharedMemoryStatusResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 39);

/**
 * @@
 * @@  .. cpp:var:: message RegionStatus
 * @@
 * @@     Status for a shared memory region.
 * @@
 *
 * @generated from message inference.CudaSharedMemoryStatusResponse.RegionStatus
 */
export type CudaSharedMemoryStatusResponse_RegionStatus = Message<"inference.CudaSharedMemoryStatusResponse.RegionStatus"> & {
  /**
   * @@
   * @@    .. cpp:var:: string name
   * @@
   * @@       The name for the shared memory region.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@    .. cpp:var:: uin64 device_id
   * @@
   * @@       The GPU device ID where the cudaIPC handle was created.
   * @@
   *
   * @generated from field: uint64 device_id = 2;
   */
  deviceId: bigint;

  /**
   * @@    .. cpp:var:: uint64 byte_size
   * @@
   * @@       Size of the shared memory region, in bytes.
   * @@
   *
   * @generated from field: uint64 byte_size = 3;
   */
  byteSize: bigint;
};

/**
 * Describes the message inference.CudaSharedMemoryStatusResponse.RegionStatus.
 * Use `create(CudaSharedMemoryStatusResponse_RegionStatusSchema)` to create a new message.
 */
export const CudaSharedMemoryStatusResponse_RegionStatusSchema: GenMessage<CudaSharedMemoryStatusResponse_RegionStatus> = /*@__PURE__*/
  messageDesc(file_grpc_service, 39, 0);

/**
 * @@
 * @@.. cpp:var:: message CudaSharedMemoryRegisterRequest
 * @@
 * @@   Request message for CudaSharedMemoryRegister.
 * @@
 *
 * @generated from message inference.CudaSharedMemoryRegisterRequest
 */
export type CudaSharedMemoryRegisterRequest = Message<"inference.CudaSharedMemoryRegisterRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the region to register.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;

  /**
   * @@  .. cpp:var:: bytes raw_handle
   * @@
   * @@     The raw serialized cudaIPC handle.
   * @@
   *
   * @generated from field: bytes raw_handle = 2;
   */
  rawHandle: Uint8Array;

  /**
   * @@  .. cpp:var:: int64 device_id
   * @@
   * @@     The GPU device ID on which the cudaIPC handle was created.
   * @@
   *
   * @generated from field: int64 device_id = 3;
   */
  deviceId: bigint;

  /**
   * @@  .. cpp:var:: uint64 byte_size
   * @@
   * @@     Size of the shared memory block, in bytes.
   * @@
   *
   * @generated from field: uint64 byte_size = 4;
   */
  byteSize: bigint;
};

/**
 * Describes the message inference.CudaSharedMemoryRegisterRequest.
 * Use `create(CudaSharedMemoryRegisterRequestSchema)` to create a new message.
 */
export const CudaSharedMemoryRegisterRequestSchema: GenMessage<CudaSharedMemoryRegisterRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 40);

/**
 * @@
 * @@.. cpp:var:: message CudaSharedMemoryRegisterResponse
 * @@
 * @@   Response message for CudaSharedMemoryRegister.
 * @@
 *
 * @generated from message inference.CudaSharedMemoryRegisterResponse
 */
export type CudaSharedMemoryRegisterResponse = Message<"inference.CudaSharedMemoryRegisterResponse"> & {
};

/**
 * Describes the message inference.CudaSharedMemoryRegisterResponse.
 * Use `create(CudaSharedMemoryRegisterResponseSchema)` to create a new message.
 */
export const CudaSharedMemoryRegisterResponseSchema: GenMessage<CudaSharedMemoryRegisterResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 41);

/**
 * @@
 * @@.. cpp:var:: message CudaSharedMemoryUnregisterRequest
 * @@
 * @@   Request message for CudaSharedMemoryUnregister.
 * @@
 *
 * @generated from message inference.CudaSharedMemoryUnregisterRequest
 */
export type CudaSharedMemoryUnregisterRequest = Message<"inference.CudaSharedMemoryUnregisterRequest"> & {
  /**
   * @@
   * @@  .. cpp:var:: string name
   * @@
   * @@     The name of the cuda region to unregister. If empty
   * @@     all cuda shared-memory regions are unregistered.
   * @@
   *
   * @generated from field: string name = 1;
   */
  name: string;
};

/**
 * Describes the message inference.CudaSharedMemoryUnregisterRequest.
 * Use `create(CudaSharedMemoryUnregisterRequestSchema)` to create a new message.
 */
export const CudaSharedMemoryUnregisterRequestSchema: GenMessage<CudaSharedMemoryUnregisterRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 42);

/**
 * @@
 * @@.. cpp:var:: message CudaSharedMemoryUnregisterResponse
 * @@
 * @@   Response message for CudaSharedMemoryUnregister.
 * @@
 *
 * @generated from message inference.CudaSharedMemoryUnregisterResponse
 */
export type CudaSharedMemoryUnregisterResponse = Message<"inference.CudaSharedMemoryUnregisterResponse"> & {
};

/**
 * Describes the message inference.CudaSharedMemoryUnregisterResponse.
 * Use `create(CudaSharedMemoryUnregisterResponseSchema)` to create a new message.
 */
export const CudaSharedMemoryUnregisterResponseSchema: GenMessage<CudaSharedMemoryUnregisterResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 43);

/**
 * @@
 * @@.. cpp:var:: message TraceSettingRequest
 * @@
 * @@   Request message for TraceSetting.
 * @@
 *
 * @generated from message inference.TraceSettingRequest
 */
export type TraceSettingRequest = Message<"inference.TraceSettingRequest"> & {
  /**
   * @@  .. cpp:var:: map<string,SettingValue> settings
   * @@
   * @@     The new setting values to be updated,
   * @@     settings that are not specified will remain unchanged.
   * @@
   *
   * @generated from field: map<string, inference.TraceSettingRequest.SettingValue> settings = 1;
   */
  settings: { [key: string]: TraceSettingRequest_SettingValue };

  /**
   * @@
   * @@  .. cpp:var:: string model_name
   * @@
   * @@     The name of the model to apply the new trace settings.
   * @@     If not given, the new settings will be applied globally.
   * @@
   *
   * @generated from field: string model_name = 2;
   */
  modelName: string;
};

/**
 * Describes the message inference.TraceSettingRequest.
 * Use `create(TraceSettingRequestSchema)` to create a new message.
 */
export const TraceSettingRequestSchema: GenMessage<TraceSettingRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 44);

/**
 * @@
 * @@  .. cpp:var:: message SettingValue
 * @@
 * @@     The values to be associated with a trace setting.
 * @@     If no value is provided, the setting will be clear and
 * @@     the global setting value will be used.
 * @@
 *
 * @generated from message inference.TraceSettingRequest.SettingValue
 */
export type TraceSettingRequest_SettingValue = Message<"inference.TraceSettingRequest.SettingValue"> & {
  /**
   * @@
   * @@    .. cpp:var:: string value (repeated)
   * @@
   * @@       The value.
   * @@
   *
   * @generated from field: repeated string value = 1;
   */
  value: string[];
};

/**
 * Describes the message inference.TraceSettingRequest.SettingValue.
 * Use `create(TraceSettingRequest_SettingValueSchema)` to create a new message.
 */
export const TraceSettingRequest_SettingValueSchema: GenMessage<TraceSettingRequest_SettingValue> = /*@__PURE__*/
  messageDesc(file_grpc_service, 44, 0);

/**
 * @@
 * @@.. cpp:var:: message TraceSettingResponse
 * @@
 * @@   Response message for TraceSetting.
 * @@
 *
 * @generated from message inference.TraceSettingResponse
 */
export type TraceSettingResponse = Message<"inference.TraceSettingResponse"> & {
  /**
   * @@  .. cpp:var:: map<string,SettingValue> settings
   * @@
   * @@     The current trace settings, including any changes specified
   * @@     by TraceSettingRequest.
   * @@
   *
   * @generated from field: map<string, inference.TraceSettingResponse.SettingValue> settings = 1;
   */
  settings: { [key: string]: TraceSettingResponse_SettingValue };
};

/**
 * Describes the message inference.TraceSettingResponse.
 * Use `create(TraceSettingResponseSchema)` to create a new message.
 */
export const TraceSettingResponseSchema: GenMessage<TraceSettingResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 45);

/**
 * @@
 * @@  .. cpp:var:: message SettingValue
 * @@
 * @@     The values to be associated with a trace setting.
 * @@
 *
 * @generated from message inference.TraceSettingResponse.SettingValue
 */
export type TraceSettingResponse_SettingValue = Message<"inference.TraceSettingResponse.SettingValue"> & {
  /**
   * @@
   * @@    .. cpp:var:: string value (repeated)
   * @@
   * @@       The value.
   * @@
   *
   * @generated from field: repeated string value = 1;
   */
  value: string[];
};

/**
 * Describes the message inference.TraceSettingResponse.SettingValue.
 * Use `create(TraceSettingResponse_SettingValueSchema)` to create a new message.
 */
export const TraceSettingResponse_SettingValueSchema: GenMessage<TraceSettingResponse_SettingValue> = /*@__PURE__*/
  messageDesc(file_grpc_service, 45, 0);

/**
 * @@
 * @@.. cpp:var:: message LogSettingsRequest
 * @@
 * @@   Request message for LogSettings.
 * @@
 *
 * @generated from message inference.LogSettingsRequest
 */
export type LogSettingsRequest = Message<"inference.LogSettingsRequest"> & {
  /**
   * @@  .. cpp:var:: map<string,SettingValue> settings
   * @@
   * @@     The current log settings.
   * @@
   *
   * @generated from field: map<string, inference.LogSettingsRequest.SettingValue> settings = 1;
   */
  settings: { [key: string]: LogSettingsRequest_SettingValue };
};

/**
 * Describes the message inference.LogSettingsRequest.
 * Use `create(LogSettingsRequestSchema)` to create a new message.
 */
export const LogSettingsRequestSchema: GenMessage<LogSettingsRequest> = /*@__PURE__*/
  messageDesc(file_grpc_service, 46);

/**
 * @generated from message inference.LogSettingsRequest.SettingValue
 */
export type LogSettingsRequest_SettingValue = Message<"inference.LogSettingsRequest.SettingValue"> & {
  /**
   * @generated from oneof inference.LogSettingsRequest.SettingValue.parameter_choice
   */
  parameterChoice: {
    /**
     * @@    .. cpp:var:: bool bool_param
     * @@
     * @@       A boolean parameter value.
     * @@
     *
     * @generated from field: bool bool_param = 1;
     */
    value: boolean;
    case: "boolParam";
  } | {
    /**
     * @@    .. cpp:var:: uint32 uint32_param
     * @@
     * @@       An uint32 parameter value.
     * @@
     *
     * @generated from field: uint32 uint32_param = 2;
     */
    value: number;
    case: "uint32Param";
  } | {
    /**
     * @@    .. cpp:var:: string string_param
     * @@
     * @@       A string parameter value.
     * @@
     *
     * @generated from field: string string_param = 3;
     */
    value: string;
    case: "stringParam";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message inference.LogSettingsRequest.SettingValue.
 * Use `create(LogSettingsRequest_SettingValueSchema)` to create a new message.
 */
export const LogSettingsRequest_SettingValueSchema: GenMessage<LogSettingsRequest_SettingValue> = /*@__PURE__*/
  messageDesc(file_grpc_service, 46, 0);

/**
 * @@
 * @@.. cpp:var:: message LogSettingsResponse
 * @@
 * @@   Response message for LogSettings.
 * @@
 *
 * @generated from message inference.LogSettingsResponse
 */
export type LogSettingsResponse = Message<"inference.LogSettingsResponse"> & {
  /**
   * @@  .. cpp:var:: map<string,SettingValue> settings
   * @@
   * @@     The current log settings.
   * @@
   *
   * @generated from field: map<string, inference.LogSettingsResponse.SettingValue> settings = 1;
   */
  settings: { [key: string]: LogSettingsResponse_SettingValue };
};

/**
 * Describes the message inference.LogSettingsResponse.
 * Use `create(LogSettingsResponseSchema)` to create a new message.
 */
export const LogSettingsResponseSchema: GenMessage<LogSettingsResponse> = /*@__PURE__*/
  messageDesc(file_grpc_service, 47);

/**
 * @generated from message inference.LogSettingsResponse.SettingValue
 */
export type LogSettingsResponse_SettingValue = Message<"inference.LogSettingsResponse.SettingValue"> & {
  /**
   * @generated from oneof inference.LogSettingsResponse.SettingValue.parameter_choice
   */
  parameterChoice: {
    /**
     * @@    .. cpp:var:: bool bool_param
     * @@
     * @@       A boolean parameter value.
     * @@
     *
     * @generated from field: bool bool_param = 1;
     */
    value: boolean;
    case: "boolParam";
  } | {
    /**
     * @@    .. cpp:var:: uint32 uint32_param
     * @@
     * @@       An int32 parameter value.
     * @@
     *
     * @generated from field: uint32 uint32_param = 2;
     */
    value: number;
    case: "uint32Param";
  } | {
    /**
     * @@    .. cpp:var:: string string_param
     * @@
     * @@       A string parameter value.
     * @@
     *
     * @generated from field: string string_param = 3;
     */
    value: string;
    case: "stringParam";
  } | { case: undefined; value?: undefined };
};

/**
 * Describes the message inference.LogSettingsResponse.SettingValue.
 * Use `create(LogSettingsResponse_SettingValueSchema)` to create a new message.
 */
export const LogSettingsResponse_SettingValueSchema: GenMessage<LogSettingsResponse_SettingValue> = /*@__PURE__*/
  messageDesc(file_grpc_service, 47, 0);

/**
 * @@
 * @@.. cpp:var:: service InferenceService
 * @@
 * @@   Inference Server GRPC endpoints.
 * @@
 *
 * @generated from service inference.GRPCInferenceService
 */
export const GRPCInferenceService: GenService<{
  /**
   * @@  .. cpp:var:: rpc ServerLive(ServerLiveRequest) returns
   * @@       (ServerLiveResponse)
   * @@
   * @@     Check liveness of the inference server.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ServerLive
   */
  serverLive: {
    methodKind: "unary";
    input: typeof ServerLiveRequestSchema;
    output: typeof ServerLiveResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ServerReady(ServerReadyRequest) returns
   * @@       (ServerReadyResponse)
   * @@
   * @@     Check readiness of the inference server.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ServerReady
   */
  serverReady: {
    methodKind: "unary";
    input: typeof ServerReadyRequestSchema;
    output: typeof ServerReadyResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ModelReady(ModelReadyRequest) returns
   * @@       (ModelReadyResponse)
   * @@
   * @@     Check readiness of a model in the inference server.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ModelReady
   */
  modelReady: {
    methodKind: "unary";
    input: typeof ModelReadyRequestSchema;
    output: typeof ModelReadyResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ServerMetadata(ServerMetadataRequest) returns
   * @@       (ServerMetadataResponse)
   * @@
   * @@     Get server metadata.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ServerMetadata
   */
  serverMetadata: {
    methodKind: "unary";
    input: typeof ServerMetadataRequestSchema;
    output: typeof ServerMetadataResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ModelMetadata(ModelMetadataRequest) returns
   * @@       (ModelMetadataResponse)
   * @@
   * @@     Get model metadata.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ModelMetadata
   */
  modelMetadata: {
    methodKind: "unary";
    input: typeof ModelMetadataRequestSchema;
    output: typeof ModelMetadataResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ModelInfer(ModelInferRequest) returns
   * @@       (ModelInferResponse)
   * @@
   * @@     Perform inference using a specific model.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ModelInfer
   */
  modelInfer: {
    methodKind: "unary";
    input: typeof ModelInferRequestSchema;
    output: typeof ModelInferResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ModelStreamInfer(stream ModelInferRequest) returns
   * @@       (stream ModelStreamInferResponse)
   * @@
   * @@     Perform streaming inference.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ModelStreamInfer
   */
  modelStreamInfer: {
    methodKind: "bidi_streaming";
    input: typeof ModelInferRequestSchema;
    output: typeof ModelStreamInferResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ModelConfig(ModelConfigRequest) returns
   * @@       (ModelConfigResponse)
   * @@
   * @@     Get model configuration.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ModelConfig
   */
  modelConfig: {
    methodKind: "unary";
    input: typeof ModelConfigRequestSchema;
    output: typeof ModelConfigResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc ModelStatistics(
   * @@                     ModelStatisticsRequest)
   * @@                   returns (ModelStatisticsResponse)
   * @@
   * @@     Get the cumulative inference statistics for a model.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.ModelStatistics
   */
  modelStatistics: {
    methodKind: "unary";
    input: typeof ModelStatisticsRequestSchema;
    output: typeof ModelStatisticsResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc RepositoryIndex(RepositoryIndexRequest) returns
   * @@       (RepositoryIndexResponse)
   * @@
   * @@     Get the index of model repository contents.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.RepositoryIndex
   */
  repositoryIndex: {
    methodKind: "unary";
    input: typeof RepositoryIndexRequestSchema;
    output: typeof RepositoryIndexResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc RepositoryModelLoad(RepositoryModelLoadRequest) returns
   * @@       (RepositoryModelLoadResponse)
   * @@
   * @@     Load or reload a model from a repository.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.RepositoryModelLoad
   */
  repositoryModelLoad: {
    methodKind: "unary";
    input: typeof RepositoryModelLoadRequestSchema;
    output: typeof RepositoryModelLoadResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc RepositoryModelUnload(RepositoryModelUnloadRequest)
   * @@       returns (RepositoryModelUnloadResponse)
   * @@
   * @@     Unload a model.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.RepositoryModelUnload
   */
  repositoryModelUnload: {
    methodKind: "unary";
    input: typeof RepositoryModelUnloadRequestSchema;
    output: typeof RepositoryModelUnloadResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc SystemSharedMemoryStatus(
   * @@                     SystemSharedMemoryStatusRequest)
   * @@                   returns (SystemSharedMemoryStatusRespose)
   * @@
   * @@     Get the status of all registered system-shared-memory regions.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.SystemSharedMemoryStatus
   */
  systemSharedMemoryStatus: {
    methodKind: "unary";
    input: typeof SystemSharedMemoryStatusRequestSchema;
    output: typeof SystemSharedMemoryStatusResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc SystemSharedMemoryRegister(
   * @@                     SystemSharedMemoryRegisterRequest)
   * @@                   returns (SystemSharedMemoryRegisterResponse)
   * @@
   * @@     Register a system-shared-memory region.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.SystemSharedMemoryRegister
   */
  systemSharedMemoryRegister: {
    methodKind: "unary";
    input: typeof SystemSharedMemoryRegisterRequestSchema;
    output: typeof SystemSharedMemoryRegisterResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc SystemSharedMemoryUnregister(
   * @@                     SystemSharedMemoryUnregisterRequest)
   * @@                   returns (SystemSharedMemoryUnregisterResponse)
   * @@
   * @@     Unregister a system-shared-memory region.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.SystemSharedMemoryUnregister
   */
  systemSharedMemoryUnregister: {
    methodKind: "unary";
    input: typeof SystemSharedMemoryUnregisterRequestSchema;
    output: typeof SystemSharedMemoryUnregisterResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc CudaSharedMemoryStatus(
   * @@                     CudaSharedMemoryStatusRequest)
   * @@                   returns (CudaSharedMemoryStatusRespose)
   * @@
   * @@     Get the status of all registered CUDA-shared-memory regions.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.CudaSharedMemoryStatus
   */
  cudaSharedMemoryStatus: {
    methodKind: "unary";
    input: typeof CudaSharedMemoryStatusRequestSchema;
    output: typeof CudaSharedMemoryStatusResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc CudaSharedMemoryRegister(
   * @@                     CudaSharedMemoryRegisterRequest)
   * @@                   returns (CudaSharedMemoryRegisterResponse)
   * @@
   * @@     Register a CUDA-shared-memory region.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.CudaSharedMemoryRegister
   */
  cudaSharedMemoryRegister: {
    methodKind: "unary";
    input: typeof CudaSharedMemoryRegisterRequestSchema;
    output: typeof CudaSharedMemoryRegisterResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc CudaSharedMemoryUnregister(
   * @@                     CudaSharedMemoryUnregisterRequest)
   * @@                   returns (CudaSharedMemoryUnregisterResponse)
   * @@
   * @@     Unregister a CUDA-shared-memory region.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.CudaSharedMemoryUnregister
   */
  cudaSharedMemoryUnregister: {
    methodKind: "unary";
    input: typeof CudaSharedMemoryUnregisterRequestSchema;
    output: typeof CudaSharedMemoryUnregisterResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc TraceSetting(TraceSettingRequest)
   * @@                   returns (TraceSettingResponse)
   * @@
   * @@     Update and get the trace setting of the Triton server.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.TraceSetting
   */
  traceSetting: {
    methodKind: "unary";
    input: typeof TraceSettingRequestSchema;
    output: typeof TraceSettingResponseSchema;
  },
  /**
   * @@  .. cpp:var:: rpc LogSettings(LogSettingsRequest)
   * @@                   returns (LogSettingsResponse)
   * @@
   * @@     Update and get the log settings of the Triton server.
   * @@
   *
   * @generated from rpc inference.GRPCInferenceService.LogSettings
   */
  logSettings: {
    methodKind: "unary";
    input: typeof LogSettingsRequestSchema;
    output: typeof LogSettingsResponseSchema;
  },
}> = /*@__PURE__*/
  serviceDesc(file_grpc_service, 0);

